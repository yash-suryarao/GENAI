{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e80332c",
   "metadata": {},
   "source": [
    "Langchain Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce436f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#1.\n",
    "\n",
    "#Basic Prompting with PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a template\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Format the prompt with a specific input\n",
    "final_prompt = prompt.format(question=\"What is soil fertility?\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85905f12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "pip install langchain langchain_core langchain-community transformers accelerate\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load Local HuggingFace Model\n",
    "# -----------------------------\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    max_new_tokens=80\n",
    ")\n",
    "\n",
    "# Wrap model for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Few-Shot Prompt Template\n",
    "# -----------------------------\n",
    "template = \"\"\"\n",
    "You are a smart tutor.\n",
    "Answer the question using the examples below.\n",
    "\n",
    "Example 1:\n",
    "Q: What is 2+2?\n",
    "A: 4\n",
    "\n",
    "Example 2:\n",
    "Q: What is the capital of France?\n",
    "A: Paris\n",
    "\n",
    "Now answer:\n",
    "Q: {question}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Build Chain\n",
    "few_shot_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=few_shot_prompt\n",
    ")\n",
    "print(few_shot_chain.run({\"question\": \"What is the capital of Germany?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447ee15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#3.\n",
    "\n",
    "!pip install -U langchain langchain-community transformers\n",
    "# Hugging Face local pipeline\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load a local Hugging Face model (example: distilgpt2)\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", max_new_tokens=100)\n",
    "\n",
    "# Wrap with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run query\n",
    "response = chain.run(\"Explain residual connections in transformers.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
