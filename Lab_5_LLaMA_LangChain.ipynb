{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4152e0cc",
   "metadata": {},
   "source": [
    "LLaMA with LangChanin Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a24779",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies (run once)\n",
    "!pip install -q langchain langchain_community transformers accelerate sentencepiece torch\n",
    "\n",
    "\n",
    "# Step 2: Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# Step 3: Load open LLaMA-like model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Step 4: Create text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=60,   # shorter generation per step\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Step 5: Wrap it with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 6: Define simpler, cleaner prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize this paragraph in one sentence:\\n{text}\\n###\"\n",
    ")\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"Based on this summary, write a simple question:\\n{summary}\\n###\"\n",
    ")\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Give a short answer to this question:\\n{question}\\n###\"\n",
    ")\n",
    "# Step 7: Create LLM chains\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"summary\")\n",
    "question_chain = LLMChain(llm=llm, prompt=question_prompt, output_key=\"question\")\n",
    "answer_chain = LLMChain(llm=llm, prompt=answer_prompt, output_key=\"answer\")\n",
    "\n",
    "# Step 8: Combine chains\n",
    "chain = SequentialChain(\n",
    "    chains=[summary_chain, question_chain, answer_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"question\", \"answer\"]\n",
    ")\n",
    "# Step 9: Input text\n",
    "text = \"\"\"Artificial Intelligence is transforming industries by automating tasks,\n",
    "enhancing decision-making, and creating new opportunities for innovation.\"\"\"\n",
    "\n",
    "# Step 10: Run the chain\n",
    "result = chain({\"text\": text})\n",
    "\n",
    "# Step 11: Display clean outputs\n",
    "print( \"Summary:\", result[\"summary\"].strip())\n",
    "print(\"Question:\", result[\"question\"].strip())\n",
    "print(\" Answer:\", result[\"answer\"].strip())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
